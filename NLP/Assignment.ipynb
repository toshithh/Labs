{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP7njE/ejXpfhI/QqKQ0zMb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U0lvWTxXZVY5","executionInfo":{"status":"ok","timestamp":1710411632901,"user_tz":-330,"elapsed":626,"user":{"displayName":"Toshith Yadav","userId":"00123641971023374433"}},"outputId":"03365bd8-e664-4a92-880d-19793201d88a"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Package treebank is already up-to-date!\n","[nltk_data] Downloading package reuters to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":16}],"source":["import nltk\n","nltk.download(\"punkt\")\n","nltk.download(\"stopwords\")\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download(\"treebank\")\n","nltk.download(\"reuters\")"]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.tag import pos_tag\n","from nltk.corpus import stopwords\n","\n","def get_inp():\n","  x = input(\"Enter text: \")\n","  return x\n","\n","def simple_pos_tagger(text):\n","  tokens = word_tokenize(text)\n","  stop_words = set(stopwords.words(\"english\"))\n","  filtered = [token for token in tokens if token.lower() not in stop_words]\n","\n","  pos_tags = pos_tag(filtered)\n","  return(pos_tags)\n","\n","tags = simple_pos_tagger(get_inp())\n","print(tags)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aAeu6FBjaiEh","executionInfo":{"status":"ok","timestamp":1710410707297,"user_tz":-330,"elapsed":13421,"user":{"displayName":"Toshith Yadav","userId":"00123641971023374433"}},"outputId":"28bbb2a8-cf30-4aca-c388-db20ffa3a46b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter text: Try this ha haha\n","[('Try', 'VB'), ('ha', 'NN'), ('haha', 'NN')]\n"]}]},{"cell_type":"code","source":["#TF-IDF\n","\n","txt = get_inp()\n","\n","def tf(txt, term):\n","  tot = len(txt.split(\" \"))\n","  num = 0\n","  i = txt.find(term)\n","  while i>-1:\n","    txt = txt[i+len(term):]\n","    print(txt)\n","    i = txt.find(term)\n","    num+=1\n","  print(f\"TF = {num/tot}\")\n","\n","tf(txt, \"ha\")\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","def calculate_tfidf(texts):\n","    vectorizer = TfidfVectorizer()\n","    tfidf_matrix = vectorizer.fit_transform(texts)\n","    feature_names = vectorizer.get_feature_names_out()\n","    return tfidf_matrix, feature_names\n","\n","\n","tfidf_matrix, feature_names = calculate_tfidf([txt])\n","print(\"\\nTF-IDF:\")\n","for col in tfidf_matrix.nonzero()[1]:\n","    print(f\"{feature_names[col]}: {tfidf_matrix[0, col]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d-v3w_pmbK57","executionInfo":{"status":"ok","timestamp":1710411480103,"user_tz":-330,"elapsed":4592,"user":{"displayName":"Toshith Yadav","userId":"00123641971023374433"}},"outputId":"8d2ed349-d6cf-4869-ae66-7eb9369caa91"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter text: ha ha ha\n"," ha ha\n"," ha\n","\n","TF = 1.0\n","\n","TF-IDF:\n","ha: 1.0\n"]}]},{"cell_type":"code","source":["import nltk\n","import random\n","from nltk.corpus import reuters\n","from nltk import bigrams, trigrams\n","from collections import Counter, defaultdict\n","from nltk.util import ngrams\n","\n","def create_ngrams(text, n):\n","    words = text.split()\n","    ngrams = []\n","    for i in range(len(words) - n + 1):\n","        ngrams.append(words[i:i + n])\n","    return ngrams\n","\n","def train_ngram_model(text, n):\n","    ngrams = create_ngrams(text, n)\n","    model = {}\n","    for ngram in ngrams:\n","        prefix = tuple(ngram[:-1])\n","        suffix = ngram[-1]\n","        if prefix not in model:\n","            model[prefix] = []\n","        model[prefix].append(suffix)\n","    return model\n","\n","def generate_text(model, seed_text, max_words):\n","    output = seed_text.split()\n","    prefix = tuple(seed_text.split()[-(n - 1):])\n","    for _ in range(max_words):\n","        if prefix in model:\n","            suffix = random.choice(model[prefix])\n","            output.append(suffix)\n","            prefix = tuple(output[-(n - 1):])\n","        else:\n","            break\n","    return ' '.join(output)\n","\n","text = get_inp()\n","n = 2\n","\n","model = train_ngram_model(text, n)\n","seed_text = \"likes\"\n","max_words = 5\n","generated_text = generate_text(model, seed_text, max_words)\n","print(generated_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mUqpCpU7eQtM","executionInfo":{"status":"ok","timestamp":1710412257144,"user_tz":-330,"elapsed":9234,"user":{"displayName":"Toshith Yadav","userId":"00123641971023374433"}},"outputId":"a6c54c7a-92e1-4681-cabd-5529b3c282a8"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter text: Madhurya likes to dance\n","likes to dance\n"]}]}]}