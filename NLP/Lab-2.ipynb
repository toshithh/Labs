{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNHZ4QmtZHapxgg9F6+Vs2q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<h1>Tokenization - POS Tagging</h1>\n","\n","- Giving each word in a text, a grammatical category such as nouns, pronouns, verbs, adjectives and adverbs.\n","\n","- Rule based POS\n","  - Rule Based Tagging: <pre>PTB</pre>\n","  - Transformation based tagging\n","  - Statistical POS Tagging:\n","  <pre>\n","  Conditional Random Fields(CRF)\n","  Hidden Markup Model(HMM)</pre>\n","\n","- <b>Penn Treebank</b>(PTB) - The English PTB corpus, and in particular the section of the corpus corresponding to the articles of Wall Street Journal (WSJ), is one of the most known and used corpus for the evaluation of models for sequence labelling. The task consists of annotating each word with its Part-of-Speech tag. In the most common split of this corpus, sections from 0 to 18 are used for training (38 219 sentences, 912 344 tokens), sections from 19 to 21 are used for validation (5 527 sentences, 131 768 tokens), and sections from 22 to 24 are used for testing (5 462 sentences, 129 654 tokens). The corpus is also commonly used for character-level and word-level Language Modelling.\n","<br>\n","\n","<h2> Tagger</h2>\n","\n","- Default tagger(Based on PTB)\n","- Unigram tagger(like machine learning, based on training dataset)\n","- Bigram tagger(Related to semantics(meaning))"],"metadata":{"id":"Q7DV7yyp0Dh8"}},{"cell_type":"code","source":["import nltk\n","nltk.download(\"punkt\")\n","nltk.download(\"stopwords\")\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download(\"treebank\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l20nBukh61U6","executionInfo":{"status":"ok","timestamp":1707368031311,"user_tz":-330,"elapsed":1711,"user":{"displayName":"Toshith Yadav","userId":"00123641971023374433"}},"outputId":"63701477-1d4e-4f7f-c215-6a1923cd5655"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/treebank.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# SAMPLE POS TAGGING\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.tag import pos_tag\n","from nltk.corpus import stopwords\n","\n","def simple_pos_tagger(text):\n","  tokens = word_tokenize(text)\n","  stop_words = set(stopwords.words(\"english\"))\n","  filtered = [token for token in tokens if token.lower() not in stop_words]\n","\n","  pos_tags = pos_tag(filtered)\n","  return(pos_tags)\n","\n","txt = \"Obama does Natural Language Processing is fascinating and not so challenging.\"\n","tags = simple_pos_tagger(txt)\n","print(tags)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CRWvt7Vx46gk","executionInfo":{"status":"ok","timestamp":1707368032237,"user_tz":-330,"elapsed":934,"user":{"displayName":"Toshith Yadav","userId":"00123641971023374433"}},"outputId":"f062fb15-0698-4520-fd27-6ac08c2c6dbe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('Obama', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('fascinating', 'VBG'), ('challenging', 'NN'), ('.', '.')]\n"]}]},{"cell_type":"code","source":["#COMPARISON OF TAGGERS\n","from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger\n","\n","def compare_taggers(text):\n","  tokens = word_tokenize(text)\n","  stop_words = set(stopwords.words(\"english\"))\n","  filtered = [token for token in tokens if token.lower() not in stop_words]\n","\n","  train_size = int(0.8*len(filtered))\n","  train, test = filtered[:train_size], filtered[:train_size]\n","  tagged_sent = [(token, 'NN') for token in train]\n","  print(\"Tagged:\", tagged_sent)\n","\n","  def_tag = DefaultTagger(\"NN\")\n","  uni_tag = UnigramTagger(model = {word: tag for word,tag in tagged_sent})\n","  bi_tag = BigramTagger(train=[tagged_sent], backoff=uni_tag)\n","\n","  gold_standard = [(token, 'NN') for token in test]\n","\n","  test_sents = [[(token, \"NN\") for token in test]]\n","\n","  tag_sent_def = def_tag.tag_sents(test_sents)\n","  tag_sent_uni = uni_tag.tag_sents(test_sents)\n","  tag_sent_bi = bi_tag.tag_sents(test_sents)\n","\n","  tag_def = [tag for (word, tag) in tag_sent_def[0]]\n","  tag_uni = [tag for (word, tag) in tag_sent_uni[0]]\n","  tag_bi = [tag for (word, tag) in tag_sent_bi[0]]\n","\n","  accuracy_def = sum(1 for gold, pred in zip(gold_standard, tag_def) if gold[1] == pred)/len(gold_standard)\n","  accuracy_uni = sum(1 for gold, pred in zip(gold_standard, tag_uni) if gold[1] == pred)/len(gold_standard)\n","  accuracy_bi = sum(1 for gold, pred in zip(gold_standard, tag_bi) if gold[1] == pred)/len(gold_standard)\n","\n","\n","  return accuracy_def, accuracy_uni, accuracy_bi\n","\n","acc = compare_taggers(txt)\n","\n","print(f\"Default: {acc[0]}\\nUnigram: {acc[1]}\\nBigram: {acc[2]}\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BXs8u2UE8z6g","executionInfo":{"status":"ok","timestamp":1707368032237,"user_tz":-330,"elapsed":4,"user":{"displayName":"Toshith Yadav","userId":"00123641971023374433"}},"outputId":"aa5f1d55-0eb8-415c-ada6-596a4b50b127"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tagged: [('Obama', 'NN'), ('Natural', 'NN'), ('Language', 'NN'), ('Processing', 'NN'), ('fascinating', 'NN')]\n","Default: 1.0\n","Unigram: 0.0\n","Bigram: 0.0\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import treebank\n","from nltk.tag import UnigramTagger, BigramTagger\n","\n","train = treebank.tagged_sents()[:2000]\n","test = treebank.tagged_sents()[2000:]\n","\n","uni_tag = UnigramTagger([tags])\n","bi_tag = BigramTagger(train, backoff=uni_tag)\n","\n","uni_tag.tag(tags)\n","print(uni_tag.evaluate([tags]))\n","print(bi_tag.evaluate([tags]))"],"metadata":{"id":"ISpg638aaO-h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707368117879,"user_tz":-330,"elapsed":2670,"user":{"displayName":"Toshith Yadav","userId":"00123641971023374433"}},"outputId":"b2db8bbb-7f2c-4f7b-e159-8bfb00b14723"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.0\n","1.0\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-8-f3d78c06d02a>:12: DeprecationWarning: \n","  Function evaluate() has been deprecated.  Use accuracy(gold)\n","  instead.\n","  print(uni_tag.evaluate([tags]))\n","<ipython-input-8-f3d78c06d02a>:13: DeprecationWarning: \n","  Function evaluate() has been deprecated.  Use accuracy(gold)\n","  instead.\n","  print(bi_tag.evaluate([tags]))\n"]}]}]}