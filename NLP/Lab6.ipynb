{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNX+Pbgrqj1Ibfr22qDA4uD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import nltk\n","nltk.download('reuters')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yFFzRPlhTtR6","executionInfo":{"status":"ok","timestamp":1710403719214,"user_tz":-330,"elapsed":1295,"user":{"displayName":"Toshith Yadav","userId":"00123641971023374433"}},"outputId":"69a72573-3a89-4ed8-b319-810e4bc64abe"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package reuters to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"--OFs9eVTVii","executionInfo":{"status":"ok","timestamp":1710403745684,"user_tz":-330,"elapsed":23234,"user":{"displayName":"Toshith Yadav","userId":"00123641971023374433"}},"outputId":"0364a442-1dc4-45ad-a1d9-c5c66e4ea6c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Generating 5 sentences of 10 words each\n","Sentence 1: today on the heavy rains but it should realized 10 , Bank\n","Sentence 2: today on a capital reorganization to allow you to evaluate a number\n","Sentence 3: today on weather - related topic on the troubled international coffee prices\n","Sentence 4: today on natural rubber over the enlargement of the Gallaher group .\n","Sentence 5: today on its behalf 2 . 40 Exports 7 . 36 PCT\n"]}],"source":["import nltk\n","import random\n","from nltk.corpus import reuters\n","from nltk import bigrams, trigrams\n","from collections import Counter, defaultdict\n","\n","\n","tokens = reuters.words()\n","\n","bi_tokens = list(bigrams(tokens))\n","tri_tokens = list(trigrams(tokens))\n","\n","bi_freq = nltk.FreqDist(bi_tokens)\n","tri_freq = nltk.FreqDist(tri_tokens)\n","\n","tri_model = defaultdict(lambda: defaultdict(lambda: 0))\n","\n","for w1, w2, w3 in tri_tokens:\n","  tri_model[(w1, w2)][w3] += 1\n","\n","def generate_text(seed_word, num_words):\n","  text = [seed_word[0], seed_word[1]]\n","  for i in range(num_words):\n","    next_word = random.choice(list(tri_model[(text[-2], text[-1])].keys()))\n","    text.append(next_word)\n","  return ' '.join(text)\n","\n","def evaluate_model(seed_word, num_sents, num_words_per_sent):\n","  print(f\"Generating {num_sents} sentences of {num_words_per_sent} words each\")\n","  for i in range(num_sents):\n","    generated_sent = generate_text(seed_word, num_words_per_sent)\n","    print(f\"Sentence {i+1}: {generated_sent}\")\n","\n","\n","seed_word = (\"today\", \"on\")\n","num_sents = 5\n","num_word_per_sent = 10\n","\n","evaluate_model(seed_word, num_sents, num_word_per_sent)"]},{"cell_type":"code","source":["from nltk.util import ngrams\n","\n","quad_tokens = list(ngrams(tokens, 4))\n","\n","quad_freq = nltk.FreqDist(quad_tokens)\n","\n","quad_model = defaultdict(lambda: defaultdict(lambda: 0))\n","\n","for w1, w2, w3, w4 in quad_tokens:\n","  quad_model[(w1, w2, w3)][w4] += 1\n","\n","\n","def generate_text(seed_word, num_words):\n","  text = list(seed_word)\n","  for i in range(num_words):\n","    next_word = random.choice(list(quad_model[(text[-3], text[-2], text[-1])].keys()))\n","    text.append(next_word)\n","  return ' '.join(text)\n","\n","seed_word = (\"today\", \"on\", \"the\")\n","num_sents = 5\n","num_word_per_sent = 15\n","\n","evaluate_model(seed_word, num_sents, num_word_per_sent)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CgXPYZ2lYmq8","executionInfo":{"status":"ok","timestamp":1710403821330,"user_tz":-330,"elapsed":10907,"user":{"displayName":"Toshith Yadav","userId":"00123641971023374433"}},"outputId":"d19a4d0a-c793-4072-a934-3bb52ea62867"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Generating 5 sentences of 15 words each\n","Sentence 1: today on the main sticking point on quotas has been Brazil ' s 20 mln dlrs versis 40\n","Sentence 2: today on the NYSE at 30 - 1 / 4 in active trading . Vague rumors that Wendy\n","Sentence 3: today on the NYSE . Reed outlined a plan to limit total exports to 90 , 000 tonnes\n","Sentence 4: today on the issue continues ,\" DiBona said at a ceremenony to sign the extension of credit by\n","Sentence 5: today on the issue will be followed by a bond of around one billion U . S later\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import random\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","text = \"\"\"The quick brown fox jumps over the lazy dog. A quick brown dog outpaces a quick fox. The dog and the fox like to run in the forest. However, the lazy dog prefers to sleep all day.\"\"\"\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([text])\n","\n","total_words = len(tokenizer.word_index) + 1\n","\n","# Create input sequences\n","input_sequences = []\n","for line in text.split('\\n'):\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    for i in range(1, len(token_list)):\n","        n_gram_sequence = token_list[:i+1]\n","        input_sequences.append(n_gram_sequence)\n","\n","# Pad sequences\n","max_sequence_len = max([len(x) for x in input_sequences])\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n","\n","# Create predictors and label\n","xs, labels = input_sequences[:,:-1], input_sequences[:,-1]\n","ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n","\n","# Define the model\n","model = Sequential()\n","model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n","model.add(LSTM(150, return_sequences=True))\n","model.add(LSTM(150))\n","model.add(Dense(total_words, activation='softmax'))\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","history = model.fit(xs, ys, epochs=100, verbose=1)\n","\n","# Function to generate text\n","def generate_text(seed_text, next_words, model, max_sequence_len):\n","    for _ in range(next_words):\n","        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","        predicted_probs = model.predict(token_list, verbose=0)[0]\n","        predicted_index = np.argmax(predicted_probs)\n","        output_word = \"\"\n","        for word, index in tokenizer.word_index.items():\n","            if index == predicted_index:\n","                output_word = word\n","                break\n","        seed_text += \" \" + output_word\n","    return seed_text\n","\n","\n","# Generate text\n","seed_text = \"fox\"\n","next_words = 5\n","generated_text = generate_text(seed_text, next_words, model, max_sequence_len)\n","print(generated_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5CTISddwbGv2","executionInfo":{"status":"ok","timestamp":1709806800797,"user_tz":-330,"elapsed":54096,"user":{"displayName":"Toshith Yadav","userId":"00123641971023374433"}},"outputId":"744bbbd0-59b1-40a2-a110-2a8e774197ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","2/2 [==============================] - 5s 70ms/step - loss: 3.0910 - accuracy: 0.0278\n","Epoch 2/100\n","2/2 [==============================] - 0s 67ms/step - loss: 3.0721 - accuracy: 0.1389\n","Epoch 3/100\n","2/2 [==============================] - 0s 58ms/step - loss: 3.0515 - accuracy: 0.0833\n","Epoch 4/100\n","2/2 [==============================] - 0s 61ms/step - loss: 3.0115 - accuracy: 0.0833\n","Epoch 5/100\n","2/2 [==============================] - 0s 66ms/step - loss: 2.9042 - accuracy: 0.1111\n","Epoch 6/100\n","2/2 [==============================] - 0s 64ms/step - loss: 2.8688 - accuracy: 0.1111\n","Epoch 7/100\n","2/2 [==============================] - 0s 78ms/step - loss: 3.0075 - accuracy: 0.1111\n","Epoch 8/100\n","2/2 [==============================] - 0s 63ms/step - loss: 2.9057 - accuracy: 0.1111\n","Epoch 9/100\n","2/2 [==============================] - 0s 69ms/step - loss: 2.7966 - accuracy: 0.1389\n","Epoch 10/100\n","2/2 [==============================] - 0s 71ms/step - loss: 2.7500 - accuracy: 0.1667\n","Epoch 11/100\n","2/2 [==============================] - 0s 65ms/step - loss: 2.7017 - accuracy: 0.1667\n","Epoch 12/100\n","2/2 [==============================] - 0s 75ms/step - loss: 2.6658 - accuracy: 0.1389\n","Epoch 13/100\n","2/2 [==============================] - 0s 62ms/step - loss: 2.6300 - accuracy: 0.1667\n","Epoch 14/100\n","2/2 [==============================] - 0s 66ms/step - loss: 2.6119 - accuracy: 0.1667\n","Epoch 15/100\n","2/2 [==============================] - 0s 66ms/step - loss: 2.5872 - accuracy: 0.1111\n","Epoch 16/100\n","2/2 [==============================] - 0s 61ms/step - loss: 2.5445 - accuracy: 0.1111\n","Epoch 17/100\n","2/2 [==============================] - 0s 77ms/step - loss: 2.5555 - accuracy: 0.1389\n","Epoch 18/100\n","2/2 [==============================] - 0s 62ms/step - loss: 2.5165 - accuracy: 0.1389\n","Epoch 19/100\n","2/2 [==============================] - 0s 63ms/step - loss: 2.4527 - accuracy: 0.1667\n","Epoch 20/100\n","2/2 [==============================] - 0s 60ms/step - loss: 2.4924 - accuracy: 0.1944\n","Epoch 21/100\n","2/2 [==============================] - 0s 60ms/step - loss: 2.4727 - accuracy: 0.2222\n","Epoch 22/100\n","2/2 [==============================] - 0s 65ms/step - loss: 2.4387 - accuracy: 0.1944\n","Epoch 23/100\n","2/2 [==============================] - 0s 60ms/step - loss: 2.4097 - accuracy: 0.1944\n","Epoch 24/100\n","2/2 [==============================] - 0s 77ms/step - loss: 2.3706 - accuracy: 0.1944\n","Epoch 25/100\n","2/2 [==============================] - 0s 64ms/step - loss: 2.3276 - accuracy: 0.1944\n","Epoch 26/100\n","2/2 [==============================] - 0s 67ms/step - loss: 2.3025 - accuracy: 0.2222\n","Epoch 27/100\n","2/2 [==============================] - 0s 78ms/step - loss: 2.2731 - accuracy: 0.1389\n","Epoch 28/100\n","2/2 [==============================] - 0s 60ms/step - loss: 2.2525 - accuracy: 0.1667\n","Epoch 29/100\n","2/2 [==============================] - 0s 60ms/step - loss: 2.2279 - accuracy: 0.1667\n","Epoch 30/100\n","2/2 [==============================] - 0s 62ms/step - loss: 2.2508 - accuracy: 0.1944\n","Epoch 31/100\n","2/2 [==============================] - 0s 59ms/step - loss: 2.2224 - accuracy: 0.2500\n","Epoch 32/100\n","2/2 [==============================] - 0s 63ms/step - loss: 2.2940 - accuracy: 0.1944\n","Epoch 33/100\n","2/2 [==============================] - 0s 60ms/step - loss: 2.3235 - accuracy: 0.1944\n","Epoch 34/100\n","2/2 [==============================] - 0s 64ms/step - loss: 2.1919 - accuracy: 0.1944\n","Epoch 35/100\n","2/2 [==============================] - 0s 60ms/step - loss: 2.1907 - accuracy: 0.1389\n","Epoch 36/100\n","2/2 [==============================] - 0s 63ms/step - loss: 2.1750 - accuracy: 0.1389\n","Epoch 37/100\n","2/2 [==============================] - 0s 61ms/step - loss: 2.1272 - accuracy: 0.2500\n","Epoch 38/100\n","2/2 [==============================] - 0s 64ms/step - loss: 2.0990 - accuracy: 0.2500\n","Epoch 39/100\n","2/2 [==============================] - 0s 64ms/step - loss: 2.1199 - accuracy: 0.1944\n","Epoch 40/100\n","2/2 [==============================] - 0s 64ms/step - loss: 2.1040 - accuracy: 0.1944\n","Epoch 41/100\n","2/2 [==============================] - 0s 59ms/step - loss: 2.0407 - accuracy: 0.2500\n","Epoch 42/100\n","2/2 [==============================] - 0s 67ms/step - loss: 2.0326 - accuracy: 0.2222\n","Epoch 43/100\n","2/2 [==============================] - 0s 97ms/step - loss: 2.0177 - accuracy: 0.2500\n","Epoch 44/100\n","2/2 [==============================] - 0s 94ms/step - loss: 2.0023 - accuracy: 0.2222\n","Epoch 45/100\n","2/2 [==============================] - 0s 104ms/step - loss: 1.9828 - accuracy: 0.2222\n","Epoch 46/100\n","2/2 [==============================] - 0s 93ms/step - loss: 1.9815 - accuracy: 0.1667\n","Epoch 47/100\n","2/2 [==============================] - 0s 96ms/step - loss: 2.0420 - accuracy: 0.1667\n","Epoch 48/100\n","2/2 [==============================] - 0s 95ms/step - loss: 1.9397 - accuracy: 0.2500\n","Epoch 49/100\n","2/2 [==============================] - 0s 109ms/step - loss: 2.0576 - accuracy: 0.2222\n","Epoch 50/100\n","2/2 [==============================] - 0s 105ms/step - loss: 1.9424 - accuracy: 0.1944\n","Epoch 51/100\n","2/2 [==============================] - 0s 85ms/step - loss: 1.9635 - accuracy: 0.2222\n","Epoch 52/100\n","2/2 [==============================] - 0s 68ms/step - loss: 1.9433 - accuracy: 0.2500\n","Epoch 53/100\n","2/2 [==============================] - 0s 65ms/step - loss: 1.9170 - accuracy: 0.2500\n","Epoch 54/100\n","2/2 [==============================] - 0s 65ms/step - loss: 1.9604 - accuracy: 0.2222\n","Epoch 55/100\n","2/2 [==============================] - 0s 77ms/step - loss: 1.9136 - accuracy: 0.2778\n","Epoch 56/100\n","2/2 [==============================] - 0s 75ms/step - loss: 1.8820 - accuracy: 0.2778\n","Epoch 57/100\n","2/2 [==============================] - 0s 66ms/step - loss: 1.8471 - accuracy: 0.2500\n","Epoch 58/100\n","2/2 [==============================] - 0s 61ms/step - loss: 1.9083 - accuracy: 0.1667\n","Epoch 59/100\n","2/2 [==============================] - 0s 57ms/step - loss: 1.8413 - accuracy: 0.2500\n","Epoch 60/100\n","2/2 [==============================] - 0s 64ms/step - loss: 1.8835 - accuracy: 0.2500\n","Epoch 61/100\n","2/2 [==============================] - 0s 71ms/step - loss: 1.8783 - accuracy: 0.2500\n","Epoch 62/100\n","2/2 [==============================] - 0s 60ms/step - loss: 1.7986 - accuracy: 0.2222\n","Epoch 63/100\n","2/2 [==============================] - 0s 62ms/step - loss: 1.8049 - accuracy: 0.2500\n","Epoch 64/100\n","2/2 [==============================] - 0s 59ms/step - loss: 1.7523 - accuracy: 0.3889\n","Epoch 65/100\n","2/2 [==============================] - 0s 65ms/step - loss: 1.8194 - accuracy: 0.3611\n","Epoch 66/100\n","2/2 [==============================] - 0s 60ms/step - loss: 1.7728 - accuracy: 0.3333\n","Epoch 67/100\n","2/2 [==============================] - 0s 65ms/step - loss: 1.8135 - accuracy: 0.2778\n","Epoch 68/100\n","2/2 [==============================] - 0s 60ms/step - loss: 1.7785 - accuracy: 0.3056\n","Epoch 69/100\n","2/2 [==============================] - 0s 59ms/step - loss: 1.8334 - accuracy: 0.3333\n","Epoch 70/100\n","2/2 [==============================] - 0s 67ms/step - loss: 1.8562 - accuracy: 0.2500\n","Epoch 71/100\n","2/2 [==============================] - 0s 63ms/step - loss: 1.7903 - accuracy: 0.3333\n","Epoch 72/100\n","2/2 [==============================] - 0s 61ms/step - loss: 1.8264 - accuracy: 0.2500\n","Epoch 73/100\n","2/2 [==============================] - 0s 58ms/step - loss: 1.7798 - accuracy: 0.2500\n","Epoch 74/100\n","2/2 [==============================] - 0s 61ms/step - loss: 1.7703 - accuracy: 0.1944\n","Epoch 75/100\n","2/2 [==============================] - 0s 74ms/step - loss: 1.7187 - accuracy: 0.3889\n","Epoch 76/100\n","2/2 [==============================] - 0s 68ms/step - loss: 1.7375 - accuracy: 0.3333\n","Epoch 77/100\n","2/2 [==============================] - 0s 64ms/step - loss: 1.7265 - accuracy: 0.3333\n","Epoch 78/100\n","2/2 [==============================] - 0s 61ms/step - loss: 1.6946 - accuracy: 0.3333\n","Epoch 79/100\n","2/2 [==============================] - 0s 60ms/step - loss: 1.6899 - accuracy: 0.3611\n","Epoch 80/100\n","2/2 [==============================] - 0s 62ms/step - loss: 1.7117 - accuracy: 0.3611\n","Epoch 81/100\n","2/2 [==============================] - 0s 76ms/step - loss: 1.6878 - accuracy: 0.3333\n","Epoch 82/100\n","2/2 [==============================] - 0s 63ms/step - loss: 1.6593 - accuracy: 0.3611\n","Epoch 83/100\n","2/2 [==============================] - 0s 60ms/step - loss: 1.7158 - accuracy: 0.3056\n","Epoch 84/100\n","2/2 [==============================] - 0s 62ms/step - loss: 1.6860 - accuracy: 0.3056\n","Epoch 85/100\n","2/2 [==============================] - 0s 64ms/step - loss: 1.6404 - accuracy: 0.3889\n","Epoch 86/100\n","2/2 [==============================] - 0s 76ms/step - loss: 1.6320 - accuracy: 0.3611\n","Epoch 87/100\n","2/2 [==============================] - 0s 68ms/step - loss: 1.6543 - accuracy: 0.3333\n","Epoch 88/100\n","2/2 [==============================] - 0s 63ms/step - loss: 1.5898 - accuracy: 0.3889\n","Epoch 89/100\n","2/2 [==============================] - 0s 60ms/step - loss: 1.6390 - accuracy: 0.2778\n","Epoch 90/100\n","2/2 [==============================] - 0s 58ms/step - loss: 1.5622 - accuracy: 0.3333\n","Epoch 91/100\n","2/2 [==============================] - 0s 74ms/step - loss: 1.5997 - accuracy: 0.3333\n","Epoch 92/100\n","2/2 [==============================] - 0s 64ms/step - loss: 1.5396 - accuracy: 0.3333\n","Epoch 93/100\n","2/2 [==============================] - 0s 62ms/step - loss: 1.5616 - accuracy: 0.4167\n","Epoch 94/100\n","2/2 [==============================] - 0s 60ms/step - loss: 1.7040 - accuracy: 0.2500\n","Epoch 95/100\n","2/2 [==============================] - 0s 70ms/step - loss: 1.5921 - accuracy: 0.3889\n","Epoch 96/100\n","2/2 [==============================] - 0s 64ms/step - loss: 1.7319 - accuracy: 0.3056\n","Epoch 97/100\n","2/2 [==============================] - 0s 64ms/step - loss: 1.6723 - accuracy: 0.3611\n","Epoch 98/100\n","2/2 [==============================] - 0s 62ms/step - loss: 1.6362 - accuracy: 0.3333\n","Epoch 99/100\n","2/2 [==============================] - 0s 70ms/step - loss: 1.7120 - accuracy: 0.2778\n","Epoch 100/100\n","2/2 [==============================] - 0s 100ms/step - loss: 1.5938 - accuracy: 0.3889\n","fox brown brown brown brown brown\n"]}]}]}